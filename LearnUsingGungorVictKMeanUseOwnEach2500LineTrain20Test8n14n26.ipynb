{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!The data file Gungor_2018_VictorianAuthorAttribution_data-train.csv can be obtained from https://archive.ics.uci.edu/ml/machine-learning-databases/00454/\n",
    "\n",
    "##input data\n",
    "import pandas as pd\n",
    "\n",
    "#note: use your own path\n",
    "path_to_datafile = '..//..//DS7004//u1720146_DS7004_courseworkCodeAndData//preparationWorks//fromDS7003_Gungor2018VictorianAuthorAttribution_NGram//Gungor_2018_VictorianAuthorAttribution_data-train.csv'\n",
    "pathToGungorVict = path_to_datafile\n",
    "gungorVictRow = pd.read_csv(pathToGungorVict, encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##form training data (2500 lines x 3) and test data (20 x 3)\n",
    "##each line about 1000 words\n",
    "#Use three authors' data:\n",
    "#author:8 Charles Dickens total lines: 6914/ 14 George Eliot 2696/ 26 Jane Austen 4441\n",
    "#each first 2500 lines for training, last 20 lines for testing. Each line has 1000 words\n",
    "for i in [14, 26, 8]:\n",
    "    allLines = gungorVictRow.loc[gungorVictRow['author'] == i]\n",
    "    lines2500 = allLines.iloc[0:2500]\n",
    "    linesLast20 = allLines.iloc[-20:]\n",
    "    try:\n",
    "        train = train.append(lines2500)\n",
    "        test = test.append(linesLast20)\n",
    "    except:\n",
    "        train = lines2500\n",
    "        test = linesLast20\n",
    "train = train.sample(frac=1, random_state=42).reset_index(drop = True) #7500 lines suffled\n",
    "test = test.sample(frac=1, random_state=42).reset_index(drop = True) #60 lines suffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import various modules for forming a string cleaning function\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist( text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:  #These three lines will not be used. Pleasesee the second parameter of this function\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the punkt tokenizer and form a sentence splitting function\n",
    "import nltk.data\n",
    "#nltk.download() #no need to use this line again after it has been used once  \n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a text into parsed sentences\n",
    "def text_to_sentences( text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call text_to_wordlist to get a list of words\n",
    "            sentences.append( text_to_wordlist( raw_sentence,               remove_stopwords )) #defined as false in text_to_wordlist\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "#function for parsing the training set\n",
    "def parsing_sentence_set(text_df):\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    print(\"Parsing sentences from training set\")\n",
    "    for text in text_df[\"text\"]:\n",
    "        sentences += text_to_sentences(text, tokenizer)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "##use the functions to form a cleaned unlabelled training set\n",
    "##for performming unsupervised learning\n",
    "sentences = parsing_sentence_set(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\n",
    "\n",
    "# Set values for the single neural network layer's various parameters\n",
    "#num_features = 300    # Word vector dimensionality                      \n",
    "#min_word_count = 40   # Minimum word count                        \n",
    "#num_workers = 4       # Number of threads to run in parallel\n",
    "#context = 10          # Context window size\n",
    "#downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 5    # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size         \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "epochs= 20             #number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize and train the model (this will take some time)\n",
    "# need to install gensim's word2vec\n",
    "from gensim.models import word2vec\n",
    "def form_model_from_sentences(sentences):\n",
    "    print(\"Training model...\")\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count,\\\n",
    "                window = context, sample = downsampling, iter = epochs)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 20:23:57,158 : INFO : collecting all words and their counts\n",
      "2020-09-01 20:23:57,159 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 20:23:58,210 : INFO : collected 9976 word types from a corpus of 7435641 raw words and 7500 sentences\n",
      "2020-09-01 20:23:58,211 : INFO : Loading a fresh vocabulary\n",
      "2020-09-01 20:23:58,236 : INFO : effective_min_count=5 retains 9940 unique words (99% of original 9976, drops 36)\n",
      "2020-09-01 20:23:58,237 : INFO : effective_min_count=5 leaves 7435530 word corpus (99% of original 7435641, drops 111)\n",
      "2020-09-01 20:23:58,264 : INFO : deleting the raw counts dictionary of 9976 items\n",
      "2020-09-01 20:23:58,264 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2020-09-01 20:23:58,265 : INFO : downsampling leaves estimated 5250158 word corpus (70.6% of prior 7435530)\n",
      "2020-09-01 20:23:58,295 : INFO : estimated required memory for 9940 words and 300 dimensions: 28826000 bytes\n",
      "2020-09-01 20:23:58,297 : INFO : resetting layer weights\n",
      "2020-09-01 20:23:59,898 : INFO : training model with 4 workers on 9940 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2020-09-01 20:24:00,909 : INFO : EPOCH 1 - PROGRESS: at 26.67% examples, 1388162 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:01,910 : INFO : EPOCH 1 - PROGRESS: at 54.00% examples, 1411102 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:02,911 : INFO : EPOCH 1 - PROGRESS: at 81.47% examples, 1421137 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:03,581 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:03,588 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:03,591 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:03,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:03,596 : INFO : EPOCH - 1 : training on 7435641 raw words (5250213 effective words) took 3.7s, 1420957 effective words/s\n",
      "2020-09-01 20:24:04,600 : INFO : EPOCH 2 - PROGRESS: at 27.20% examples, 1425276 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:05,608 : INFO : EPOCH 2 - PROGRESS: at 54.93% examples, 1435262 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:06,608 : INFO : EPOCH 2 - PROGRESS: at 81.87% examples, 1428056 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:07,245 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:07,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:07,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:07,256 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:07,258 : INFO : EPOCH - 2 : training on 7435641 raw words (5248823 effective words) took 3.7s, 1434479 effective words/s\n",
      "2020-09-01 20:24:08,271 : INFO : EPOCH 3 - PROGRESS: at 27.73% examples, 1442806 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:09,276 : INFO : EPOCH 3 - PROGRESS: at 55.47% examples, 1444619 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:10,280 : INFO : EPOCH 3 - PROGRESS: at 83.33% examples, 1448990 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:10,866 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:10,867 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:10,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:10,875 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:10,876 : INFO : EPOCH - 3 : training on 7435641 raw words (5250345 effective words) took 3.6s, 1452535 effective words/s\n",
      "2020-09-01 20:24:11,878 : INFO : EPOCH 4 - PROGRESS: at 27.47% examples, 1439419 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:12,878 : INFO : EPOCH 4 - PROGRESS: at 54.27% examples, 1422527 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:13,880 : INFO : EPOCH 4 - PROGRESS: at 81.20% examples, 1419742 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:14,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:14,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:14,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:14,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:14,581 : INFO : EPOCH - 4 : training on 7435641 raw words (5250026 effective words) took 3.7s, 1417687 effective words/s\n",
      "2020-09-01 20:24:15,584 : INFO : EPOCH 5 - PROGRESS: at 26.27% examples, 1377162 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:16,587 : INFO : EPOCH 5 - PROGRESS: at 52.40% examples, 1372034 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:17,588 : INFO : EPOCH 5 - PROGRESS: at 79.47% examples, 1388488 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:18,353 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:18,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:18,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:18,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:18,367 : INFO : EPOCH - 5 : training on 7435641 raw words (5251335 effective words) took 3.8s, 1388006 effective words/s\n",
      "2020-09-01 20:24:19,372 : INFO : EPOCH 6 - PROGRESS: at 26.13% examples, 1365832 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:20,374 : INFO : EPOCH 6 - PROGRESS: at 51.60% examples, 1349403 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:21,377 : INFO : EPOCH 6 - PROGRESS: at 77.33% examples, 1348868 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:22,254 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:22,263 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:22,266 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:22,272 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:22,273 : INFO : EPOCH - 6 : training on 7435641 raw words (5249431 effective words) took 3.9s, 1344374 effective words/s\n",
      "2020-09-01 20:24:23,276 : INFO : EPOCH 7 - PROGRESS: at 25.33% examples, 1327380 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:24,278 : INFO : EPOCH 7 - PROGRESS: at 50.27% examples, 1316943 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:25,283 : INFO : EPOCH 7 - PROGRESS: at 76.00% examples, 1326036 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:26,189 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:26,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:26,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:26,205 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:26,206 : INFO : EPOCH - 7 : training on 7435641 raw words (5249023 effective words) took 3.9s, 1335733 effective words/s\n",
      "2020-09-01 20:24:27,213 : INFO : EPOCH 8 - PROGRESS: at 24.67% examples, 1286597 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:28,216 : INFO : EPOCH 8 - PROGRESS: at 49.47% examples, 1292676 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:29,218 : INFO : EPOCH 8 - PROGRESS: at 74.13% examples, 1292743 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:30,220 : INFO : EPOCH 8 - PROGRESS: at 98.80% examples, 1292576 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:30,248 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:30,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:30,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:30,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:30,267 : INFO : EPOCH - 8 : training on 7435641 raw words (5249687 effective words) took 4.1s, 1293280 effective words/s\n",
      "2020-09-01 20:24:31,271 : INFO : EPOCH 9 - PROGRESS: at 24.67% examples, 1291356 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:32,275 : INFO : EPOCH 9 - PROGRESS: at 49.33% examples, 1290378 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:33,286 : INFO : EPOCH 9 - PROGRESS: at 73.87% examples, 1285132 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 20:24:34,287 : INFO : EPOCH 9 - PROGRESS: at 98.27% examples, 1283736 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:34,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:34,349 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:34,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:34,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:34,356 : INFO : EPOCH - 9 : training on 7435641 raw words (5250152 effective words) took 4.1s, 1284629 effective words/s\n",
      "2020-09-01 20:24:35,360 : INFO : EPOCH 10 - PROGRESS: at 23.87% examples, 1250470 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:36,363 : INFO : EPOCH 10 - PROGRESS: at 48.27% examples, 1263791 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:37,367 : INFO : EPOCH 10 - PROGRESS: at 72.93% examples, 1272515 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:38,373 : INFO : EPOCH 10 - PROGRESS: at 96.93% examples, 1267371 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:38,485 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:38,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:38,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:38,498 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:38,499 : INFO : EPOCH - 10 : training on 7435641 raw words (5249452 effective words) took 4.1s, 1267972 effective words/s\n",
      "2020-09-01 20:24:39,515 : INFO : EPOCH 11 - PROGRESS: at 24.27% examples, 1255323 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:40,519 : INFO : EPOCH 11 - PROGRESS: at 48.40% examples, 1257917 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:41,521 : INFO : EPOCH 11 - PROGRESS: at 73.47% examples, 1276531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:42,535 : INFO : EPOCH 11 - PROGRESS: at 96.93% examples, 1261225 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:42,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:42,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:42,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:42,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:42,744 : INFO : EPOCH - 11 : training on 7435641 raw words (5249029 effective words) took 4.2s, 1236894 effective words/s\n",
      "2020-09-01 20:24:43,752 : INFO : EPOCH 12 - PROGRESS: at 13.73% examples, 718879 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:44,754 : INFO : EPOCH 12 - PROGRESS: at 28.00% examples, 732729 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:45,756 : INFO : EPOCH 12 - PROGRESS: at 49.07% examples, 856388 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-01 20:24:46,768 : INFO : EPOCH 12 - PROGRESS: at 74.13% examples, 968291 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:47,776 : INFO : EPOCH 12 - PROGRESS: at 99.60% examples, 1040065 words/s, in_qsize 3, out_qsize 1\n",
      "2020-09-01 20:24:47,777 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:47,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:47,783 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:47,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:47,786 : INFO : EPOCH - 12 : training on 7435641 raw words (5249821 effective words) took 5.0s, 1042236 effective words/s\n",
      "2020-09-01 20:24:48,790 : INFO : EPOCH 13 - PROGRESS: at 25.07% examples, 1312976 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:49,795 : INFO : EPOCH 13 - PROGRESS: at 49.87% examples, 1304460 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:50,796 : INFO : EPOCH 13 - PROGRESS: at 74.13% examples, 1293790 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:51,804 : INFO : EPOCH 13 - PROGRESS: at 98.67% examples, 1289999 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:51,863 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:51,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:51,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:51,884 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:51,887 : INFO : EPOCH - 13 : training on 7435641 raw words (5250217 effective words) took 4.1s, 1281078 effective words/s\n",
      "2020-09-01 20:24:52,890 : INFO : EPOCH 14 - PROGRESS: at 13.33% examples, 699012 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:53,896 : INFO : EPOCH 14 - PROGRESS: at 26.67% examples, 697487 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:54,897 : INFO : EPOCH 14 - PROGRESS: at 40.93% examples, 714484 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:55,899 : INFO : EPOCH 14 - PROGRESS: at 66.53% examples, 871023 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:56,901 : INFO : EPOCH 14 - PROGRESS: at 91.73% examples, 961171 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:57,211 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:24:57,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:24:57,228 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:24:57,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:24:57,230 : INFO : EPOCH - 14 : training on 7435641 raw words (5250468 effective words) took 5.3s, 983352 effective words/s\n",
      "2020-09-01 20:24:58,232 : INFO : EPOCH 15 - PROGRESS: at 22.40% examples, 1175104 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:24:59,233 : INFO : EPOCH 15 - PROGRESS: at 45.60% examples, 1196252 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:00,235 : INFO : EPOCH 15 - PROGRESS: at 69.33% examples, 1212143 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:01,245 : INFO : EPOCH 15 - PROGRESS: at 89.60% examples, 1172183 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:02,025 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:25:02,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:25:02,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:25:02,047 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:25:02,048 : INFO : EPOCH - 15 : training on 7435641 raw words (5249615 effective words) took 4.8s, 1089971 effective words/s\n",
      "2020-09-01 20:25:03,056 : INFO : EPOCH 16 - PROGRESS: at 12.27% examples, 641953 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:04,057 : INFO : EPOCH 16 - PROGRESS: at 29.07% examples, 761270 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:05,062 : INFO : EPOCH 16 - PROGRESS: at 53.47% examples, 933206 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:06,067 : INFO : EPOCH 16 - PROGRESS: at 77.33% examples, 1011670 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:06,945 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:25:06,949 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:25:06,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:25:06,961 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:25:06,962 : INFO : EPOCH - 16 : training on 7435641 raw words (5251692 effective words) took 4.9s, 1069905 effective words/s\n",
      "2020-09-01 20:25:07,966 : INFO : EPOCH 17 - PROGRESS: at 24.13% examples, 1264044 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:08,972 : INFO : EPOCH 17 - PROGRESS: at 48.13% examples, 1257818 words/s, in_qsize 7, out_qsize 1\n",
      "2020-09-01 20:25:09,978 : INFO : EPOCH 17 - PROGRESS: at 62.93% examples, 1096195 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:10,982 : INFO : EPOCH 17 - PROGRESS: at 76.00% examples, 992844 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-01 20:25:11,988 : INFO : EPOCH 17 - PROGRESS: at 90.00% examples, 940360 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:12,427 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 20:25:12,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:25:12,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:25:12,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:25:12,445 : INFO : EPOCH - 17 : training on 7435641 raw words (5250304 effective words) took 5.5s, 957848 effective words/s\n",
      "2020-09-01 20:25:13,456 : INFO : EPOCH 18 - PROGRESS: at 24.80% examples, 1290412 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:14,464 : INFO : EPOCH 18 - PROGRESS: at 50.40% examples, 1312295 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:15,467 : INFO : EPOCH 18 - PROGRESS: at 76.13% examples, 1323980 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:16,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:25:16,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:25:16,386 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:25:16,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:25:16,392 : INFO : EPOCH - 18 : training on 7435641 raw words (5250869 effective words) took 3.9s, 1330964 effective words/s\n",
      "2020-09-01 20:25:17,399 : INFO : EPOCH 19 - PROGRESS: at 18.00% examples, 940821 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:18,404 : INFO : EPOCH 19 - PROGRESS: at 31.87% examples, 832328 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:19,415 : INFO : EPOCH 19 - PROGRESS: at 44.80% examples, 778683 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:20,422 : INFO : EPOCH 19 - PROGRESS: at 66.40% examples, 865470 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:21,425 : INFO : EPOCH 19 - PROGRESS: at 92.53% examples, 965794 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-01 20:25:21,700 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:25:21,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:25:21,709 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:25:21,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:25:21,713 : INFO : EPOCH - 19 : training on 7435641 raw words (5250193 effective words) took 5.3s, 987293 effective words/s\n",
      "2020-09-01 20:25:22,722 : INFO : EPOCH 20 - PROGRESS: at 25.73% examples, 1343314 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:23,724 : INFO : EPOCH 20 - PROGRESS: at 51.73% examples, 1352734 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:24,742 : INFO : EPOCH 20 - PROGRESS: at 67.33% examples, 1167857 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:25,747 : INFO : EPOCH 20 - PROGRESS: at 81.07% examples, 1055708 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:26,751 : INFO : EPOCH 20 - PROGRESS: at 95.60% examples, 996914 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-01 20:25:27,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-01 20:25:27,036 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-01 20:25:27,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-01 20:25:27,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-01 20:25:27,046 : INFO : EPOCH - 20 : training on 7435641 raw words (5249991 effective words) took 5.3s, 985186 effective words/s\n",
      "2020-09-01 20:25:27,046 : INFO : training on a 148712820 raw words (105000686 effective words) took 87.1s, 1204860 effective words/s\n",
      "2020-09-01 20:25:27,047 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "##form the word2vec model with the training set which will be\n",
    "##used in the following two methods:\n",
    "##vector averaging and vector clustering of stop words\n",
    "model = form_model_from_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.3711814284324646), ('conqueror', 0.33052319288253784), ('earl', 0.30649304389953613), ('child', 0.29471921920776367), ('throne', 0.28856751322746277), ('girl', 0.28016096353530884), ('saxon', 0.2756054103374481), ('kings', 0.2677653133869171), ('knights', 0.26159268617630005), ('reign', 0.25792354345321655)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-e2296c31c599>:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(model.most_similar(positive=['king', 'woman'], negative=['man']))\n"
     ]
    }
   ],
   "source": [
    "##check the model\n",
    "# king - man + woman ~= queen\n",
    "print(model.most_similar(positive=['king', 'woman'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-15a160119dc2>:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive=['better', 'bad'], negative=['good'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('worse', 0.38985705375671387),\n",
       " ('wiser', 0.37705114483833313),\n",
       " ('sooner', 0.3492392301559448),\n",
       " ('happier', 0.3015991449356079),\n",
       " ('bigger', 0.2892671227455139),\n",
       " ('fairer', 0.2862151265144348),\n",
       " ('apprehended', 0.2851130962371826),\n",
       " ('easier', 0.27977824211120605),\n",
       " ('fewer', 0.27307164669036865),\n",
       " ('liad', 0.260669469833374)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##check the model\n",
    "# better - good + bad ~= worse\n",
    "model.most_similar(positive=['better', 'bad'], negative=['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "##first method: vector averaging of stop words:\n",
    "import gensim\n",
    "all_stopwords = set(gensim.parsing.preprocessing.STOPWORDS)\n",
    "\n",
    "#be careful: nword and counter must be integers --Chiu\n",
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph which are stop words\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    index2word_set2 = all_stopwords\n",
    "    #\n",
    "    # Loop over each word in the text and, if it is in the model's\n",
    "    # vocaublary and is a stop word add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: #and word in index2word_set2: \n",
    "            if word in index2word_set2:\n",
    "                nwords = nwords + 1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0:\n",
    "        nwords = 1 #avoid devided by zero (i.e. no stop word)\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(texts, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(texts),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the texts\n",
    "    for text in texts:\n",
    "       #\n",
    "       # Print a status message every 100th text\n",
    "        if counter%100 == 0:\n",
    "            haha = counter; hihi = len(texts)\n",
    "            print(f\"Text {haha} of {hihi}\") #% (counter, len(texts))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        #textFeatureVecs[counter] = makeFeatureVec(text, model, num_features)\n",
    "        textFeatureVecs[counter] = makeFeatureVec(text, model, num_features)\n",
    "       # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0 of 7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-7a901c89f86d>:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec, model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 100 of 7500\n",
      "Text 200 of 7500\n",
      "Text 300 of 7500\n",
      "Text 400 of 7500\n",
      "Text 500 of 7500\n",
      "Text 600 of 7500\n",
      "Text 700 of 7500\n",
      "Text 800 of 7500\n",
      "Text 900 of 7500\n",
      "Text 1000 of 7500\n",
      "Text 1100 of 7500\n",
      "Text 1200 of 7500\n",
      "Text 1300 of 7500\n",
      "Text 1400 of 7500\n",
      "Text 1500 of 7500\n",
      "Text 1600 of 7500\n",
      "Text 1700 of 7500\n",
      "Text 1800 of 7500\n",
      "Text 1900 of 7500\n",
      "Text 2000 of 7500\n",
      "Text 2100 of 7500\n",
      "Text 2200 of 7500\n",
      "Text 2300 of 7500\n",
      "Text 2400 of 7500\n",
      "Text 2500 of 7500\n",
      "Text 2600 of 7500\n",
      "Text 2700 of 7500\n",
      "Text 2800 of 7500\n",
      "Text 2900 of 7500\n",
      "Text 3000 of 7500\n",
      "Text 3100 of 7500\n",
      "Text 3200 of 7500\n",
      "Text 3300 of 7500\n",
      "Text 3400 of 7500\n",
      "Text 3500 of 7500\n",
      "Text 3600 of 7500\n",
      "Text 3700 of 7500\n",
      "Text 3800 of 7500\n",
      "Text 3900 of 7500\n",
      "Text 4000 of 7500\n",
      "Text 4100 of 7500\n",
      "Text 4200 of 7500\n",
      "Text 4300 of 7500\n",
      "Text 4400 of 7500\n",
      "Text 4500 of 7500\n",
      "Text 4600 of 7500\n",
      "Text 4700 of 7500\n",
      "Text 4800 of 7500\n",
      "Text 4900 of 7500\n",
      "Text 5000 of 7500\n",
      "Text 5100 of 7500\n",
      "Text 5200 of 7500\n",
      "Text 5300 of 7500\n",
      "Text 5400 of 7500\n",
      "Text 5500 of 7500\n",
      "Text 5600 of 7500\n",
      "Text 5700 of 7500\n",
      "Text 5800 of 7500\n",
      "Text 5900 of 7500\n",
      "Text 6000 of 7500\n",
      "Text 6100 of 7500\n",
      "Text 6200 of 7500\n",
      "Text 6300 of 7500\n",
      "Text 6400 of 7500\n",
      "Text 6500 of 7500\n",
      "Text 6600 of 7500\n",
      "Text 6700 of 7500\n",
      "Text 6800 of 7500\n",
      "Text 6900 of 7500\n",
      "Text 7000 of 7500\n",
      "Text 7100 of 7500\n",
      "Text 7200 of 7500\n",
      "Text 7300 of 7500\n",
      "Text 7400 of 7500\n",
      "Creating average feature vecs for test texts\n",
      "Text 0 of 60\n",
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. \n",
    "\n",
    "clean_train_texts = []\n",
    "for text in train[\"text\"]:\n",
    "    #clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        #remove_stopwords=True )) #do not remove stop words\n",
    "    clean_train_texts.append( text_to_wordlist( text ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_texts, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test texts\")\n",
    "clean_test_texts = []\n",
    "for text in test[\"text\"]:\n",
    "    #clean_test_texts.append( text_to_wordlist( review, remove_stopwords=True ))\n",
    "    clean_test_texts.append( text_to_wordlist( text ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_texts, model, num_features )\n",
    "\n",
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"author\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"true_author\":test[\"author\"], \"pred_author\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " Predicted  8   14  26\n",
      "Actural              \n",
      "8          16   4   0\n",
      "14          2  18   0\n",
      "26          1   1  18\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(output['true_author'], output['pred_author'], rownames=['Actural'], colnames=['Predicted'])\n",
    "print('Confusion matrix:\\n', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy: ', metrics.accuracy_score(output['true_author'], output['pred_author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8683635146303444\n"
     ]
    }
   ],
   "source": [
    "print('f1 score: ', metrics.f1_score(output['true_author'], output['pred_author'], average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-9091e8a1385b>:11: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  word_vectors = model.wv.syn0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  515.2460813522339 seconds.\n",
      "\n",
      "Cluster 0\n",
      "['fragments', 'roots', 'territory']\n",
      "\n",
      "Cluster 1\n",
      "['confined', 'indulged', 'pictured', 'sweetheart']\n",
      "\n",
      "Cluster 2\n",
      "['gold', 'silver', 'sword', 'nail', 'cord']\n",
      "\n",
      "Cluster 3\n",
      "['proceeding', 'threatened', 'assuredly', 'indirectly', 'practically']\n",
      "\n",
      "Cluster 4\n",
      "['thrown', 'cast', 'pushed', 'tumbled', 'tossed', 'dragged', 'flung', 'dragging', 'ladder', 'burned', 'kicked', 'drives', 'slide', 'precipice']\n",
      "\n",
      "Cluster 5\n",
      "['liis', 'lis']\n",
      "\n",
      "Cluster 6\n",
      "['rank', 'station', 'fashion', 'proportion', 'standard', 'equals']\n",
      "\n",
      "Cluster 7\n",
      "['trouble', 'pain', 'pang']\n",
      "\n",
      "Cluster 8\n",
      "['spreading', 'vanishing']\n",
      "\n",
      "Cluster 9\n",
      "['iii', 'sm', 'rom']\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "##second method: vector clustering of stop words (use KMeans):\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time (several to tens of minutes)\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = int(num_clusters) )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "#a cluster number\n",
    "\n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "\n",
    "# For the first 10 clusters\n",
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    #print \"\\nCluster %d\" #% cluster\n",
    "    print(f\"\\nCluster {cluster}\")\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    a_view = word_centroid_map.items()\n",
    "    tuples = list(a_view)\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        if( tuples[i][1] == cluster ):\n",
    "            words.append(tuples[i][0])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map and word in all_stopwords:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"text\"].size, int(num_clusters)),     dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for text in clean_train_texts:\n",
    "    train_centroids[counter] = create_bag_of_centroids( text,         word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros((test[\"text\"].size, int(num_clusters)),     dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for text in clean_test_texts:\n",
    "    test_centroids[counter] = create_bag_of_centroids( text,         word_centroid_map )\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# This cell take some minutes\n",
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"author\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"true_author\":test[\"author\"], \"pred_author\":result})\n",
    "output.to_csv( \"BagOfCentroidsAuthor.csv\", index=False, quoting=3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " Predicted  8   14  26\n",
      "Actural              \n",
      "8          18   2   0\n",
      "14          1  19   0\n",
      "26          0   0  20\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(output['true_author'], output['pred_author'], rownames=['Actural'], colnames=['Predicted'])\n",
    "print('Confusion matrix:\\n', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy: ', metrics.accuracy_score(output['true_author'], output['pred_author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9499687304565353\n"
     ]
    }
   ],
   "source": [
    "print('f1 score: ', metrics.f1_score(output['true_author'], output['pred_author'], average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
